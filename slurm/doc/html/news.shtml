<!--#include virtual="header.txt"-->

<h1>What's New</h1>

<h2>Index</h2>
<ul>
<li><a href="#1403">Slurm Version 14.03, March 2014</a></li>
<li><a href="#1411">Slurm Version 14.11, November 2014</a></li>
<li><a href="#1508">Slurm Version 15.08 and beyond</a></li>
</ul>

<h2><a name="1403">Major Updates in Slurm Version 14.03</a></h2>
<p>SLURM Version 14.03 was released in March 2014.
Major enhancements include:
<ul>
<li>Added support for native Slurm operation on Cray systems (without ALPS).</li>
<li>Added partition configuration parameters AllowAccounts, AllowQOS,
    DenyAccounts and DenyQOS to provide greater control over use.</li>
<li>Added the ability to perform load based scheduling. Allocating resources to
    jobs on the nodes with the largest number if idle CPUs.</li>
<li>Added support for reserving cores on a compute node for system services
    (core specialization).</li>
<li>Add mechanism for job_submit plugin to generate error message for srun,
    salloc or sbatch to stderr.</li>
<li>Added new structures and support for both server and cluster resources.</li>
<li>Significant performance improvements, especially with respect to job
    array support.</li>
<li>Improved user support for fault-tolerance (e.g. "hot spare" resources).</li>
</ul>
</p>

<h2><a name="1411">Major Updates in Slurm Version 14.11</a></h2>
<p>SLURM Version 14.11 is scheduled for release in November 2014.
Major enhancements include:
<ul>
<li>Communication gateway nodes to improve scalability.</li>
<li>Support for heterogeneous generic resources (i.e. user specification of
    desired GPU types).</li>
<li>Support for non-consumable generic resources that are shared, but limited
    in number.</li>
<li>Support for automatic job requeue policy based on exit value.</li>
<li>Add user options to set the CPU governor (OnDemand, Performance,
    PowerSave or UserSpace) in addition to being able to explicitly set the
    CPU frequency currently available.</li>
<li>Report Slurm message traffic by user, type, count and time consumed.</li>
</ul>
</p>


<h2><a name="1508">Major Updates in Slurm Version 15.08 and beyond</a></h2>
<p> Detailed plans for release dates and contents of additional SLURM releases
have not been finalized. Anyone desiring to perform SLURM development should
notify <a href="mailto:slurm-dev@schedmd.com">slurm-dev@schedmd.com</a>
to coordinate activities. Future development plans includes:
<ul>
<li>Integration with
    <a href="http://en.wikipedia.org/wiki/FlexNet_Publisher">FLEXlm (Flexnet Publisher)</a>
    license management.</li>
<li>Layouts framework, which will be the basis for further developments toward
    optimizing scheduling with respect to additional parameters such as temperature
    and power consumption.</li>
<li>Energy consumption added as a factor in fair-share scheduling.</li>
<li>Energy aware scheduling added with respect to power caps.</li>
<li>Distributed architecture to support the management of resources with Intel
    MIC processors.</li>
<li>Support of I/O as a new resources, including proxy I/O nodes with data
    locality.</li>
<li>Improved scheduling support for job dependencies (e.g. pre-processing,
    post-processing, co-processing on I/O nodes, etc.) to optimize overall
    system utilization.</li>
<li>IP communications over InfiniBand network for improved performance.</li>
<li>Fault-tolerance and jobs dynamic adaptation through communication protocol
    between Slurm , MPI libraries and the application.</li>
<li>Improved support for high-throughput computing (e.g. multiple slurmctld
    daemons on a single cluster).</li>
<li>Scheduling fully optimized for energy efficiency.</li>
<li>Numerous enhancements to advanced resource reservations (e.g. start or
    end the reservation early depending upon the workload).</li>
<li>Add Kerberos credential support including credential forwarding
    and refresh.</li>
<li>Improved support for provisioning and virtualization.</li> 
<li>Provide a web-based SLURM administration tool.</li>
</ul>

<p style="text-align:center;">Last modified 29 May 2014</p>

<!--#include virtual="footer.txt"-->
